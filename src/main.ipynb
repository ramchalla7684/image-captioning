{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSE 575: Statistical Machine Learning\n",
    "\n",
    "#### Under the guidance of \n",
    "### Prof. Guoliang Xue\n",
    "\n",
    "\n",
    "\n",
    "#### Authors: Ramulu Reddy Challa, Akhilesh Reddy Eppa, Nagarjuna Vemuri, Vrushabh Jambhulkar, Mohammed Sauban Mussaddique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Only run the cells needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy\n",
    "import matplotlib.pyplot as plot\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as process_input_inception\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as process_input_vgg\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.path.abspath(\"drive/My Drive/Colab Notebooks/image-captioning\")\n",
    "\n",
    "#Source: https://www.kaggle.com/hsankesara/flickr-image-dataset\n",
    "#Source: http://academictorrents.com/details/9dea07ba660a722ae1008c4c8afdd303b6f6e53b\n",
    "# change this if using Flickr30k dataset\n",
    "IMAGES_PATH = os.path.join(BASE_PATH, \"data/Flickr8k/Flicker8k_Dataset/\")\n",
    "TRAIN_IMAGES_PATH = os.path.join(BASE_PATH, \"data/Flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt\")\n",
    "TEST_IMAGES_PATH = os.path.join(BASE_PATH, \"data/Flickr8k/Flickr8k_text/Flickr_8k.testImages.txt\")\n",
    "RAW_CAPTION_PATH = os.path.join(BASE_PATH, \"data/Flickr8k/Flickr8k_text/Flickr8k.token.txt\")\n",
    "VOCABULARY_PATH = os.path.join(BASE_PATH, \"data/Flickr8k/vocabulary.pkl\")\n",
    "\n",
    "# Source: https://github.com/stanfordnlp/GloVe\n",
    "GLOVE_WORD_EMBEDDINGS_PATH_100D = os.path.join(BASE_PATH, \"data/glove/glove.6B.100d.txt\")\n",
    "GLOVE_WORD_EMBEDDINGS_PATH_200D = os.path.join(BASE_PATH, \"data/glove/glove.6B.200d.txt\")\n",
    "GLOVE_WORD_EMBEDDINGS_PATH_300D = os.path.join(BASE_PATH, \"data/glove/glove.6B.300d.txt\")\n",
    "\n",
    "# Paths to save files for future use (change names to reflect the models or datasets used)\n",
    "PROCESSED_CAPTIONS_PATH = os.path.join(BASE_PATH, \"data/preprocessed_captions.pkl\")\n",
    "WORD_TO_INDEX_PATH = os.path.abspath(\"../data/word_to_index.pkl\")\n",
    "INDEX_TO_WORD_PATH = os.path.abspath(\"../data/word_to_index.pkl\")\n",
    "ENCODINGS_PATH = os.path.join(BASE_PATH, \"data/encodings.pkl\")\n",
    "G_WORD_EMBEDDINGS_PATH_100D = os.path.join(BASE_PATH, \"data/Flickr8k/g_word_embeddings_100d.pkl\")\n",
    "G_WORD_EMBEDDINGS_PATH_200D = os.path.join(BASE_PATH, \"data/Flickr8k/g_word_embeddings_200d.pkl\")\n",
    "G_WORD_EMBEDDINGS_PATH_300D = os.path.join(BASE_PATH, \"data/Flickr8k/g_word_embeddings_300d.pkl\")\n",
    "TEST_RESULTS_PATH = os.path.join(BASE_PATH, \"res-inception-lstm-all-glove.pkl\")\n",
    "\n",
    "\n",
    "# Paths to load the saved models from (change names to reflect the models or datasets used)\n",
    "MODEL_PATH = os.path.join(BASE_PATH, \"inception-lstm-flickr8k-glove300d.h5\")\n",
    "MODEL_HISTORY_PATH = os.path.join(BASE_PATH, \"inception-lstm-flickr8k-glove300d-history.pkl\")\n",
    "\n",
    "MIN_WORD_FREQ = 8\n",
    "GLOVE_WORD_EMBEDDINGS_PATH = GLOVE_WORD_EMBEDDINGS_PATH_100D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting captions from Flickr8k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = {}\n",
    "with open(RAW_CAPTION_PATH, \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "#         print(line)\n",
    "        splits = re.compile(\"#\\d+\").split(line)\n",
    "        image_name = splits[0]\n",
    "        caption = splits[1]\n",
    "        caption = re.sub(r\"\\n+|\\t+|\\s{2,}\", \"\", caption)\n",
    "        if image_name not in captions:\n",
    "            captions[image_name] = []\n",
    "        captions[image_name].append(caption)\n",
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting captions from flickr30k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_names = set(os.listdir(IMAGES_PATH))\n",
    "\n",
    "captions = {}\n",
    "with open(RAW_CAPTION_PATH, \"r\") as file:\n",
    "    file.readline()\n",
    "    for line in file.readlines():\n",
    "#         print(line)\n",
    "        splits = line.split(\"|\")\n",
    "        if(len(splits) != 3):\n",
    "            continue\n",
    "        image_name = splits[0].strip()\n",
    "        if(image_name not in all_image_names):\n",
    "            continue\n",
    "        caption = splits[2].strip()\n",
    "        caption = re.sub(r\"\\n+|\\t+|\\s{2,}\", \"\", caption)\n",
    "        if image_name not in captions:\n",
    "            captions[image_name] = []\n",
    "        captions[image_name].append(caption)\n",
    "# captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample display of images along with their captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in ['1007129816_e794419615.jpg', '1002674143_1b742ab4b8.jpg', '1022454428_b6b660a67b.jpg']:\n",
    "    plot.imshow(plot.imread(os.path.join(IMAGES_PATH, img)))\n",
    "    plot.axis('off')\n",
    "    plot.show()\n",
    "    print(\"\\n\".join(captions[img]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To lowercase <br> Remove words with special characters and numbers <br> Remove special characters (punctuations) & words of unit length (might keep 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFilter(word):\n",
    "    if not re.match(r\"^[a-z]+$\", word):\n",
    "        return False\n",
    "    \n",
    "    if len(word) == 1 and word != \"a\":\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [] \n",
    "for _, _captions in captions.items():\n",
    "    for i in range(len(_captions)):\n",
    "        caption = _captions[i]\n",
    "        caption = caption.lower()\n",
    "        _words = caption.split(\" \")\n",
    "        _words = list(filter(wordFilter, _words))\n",
    "        _captions[i] = ' '.join(_words)\n",
    "#         We will add <start> and <end> in data_generator\n",
    "        _captions[i] = '<start> ' + _captions[i] + ' <end>' \n",
    "        words.extend(_words)\n",
    "word_freq = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq\n",
    "captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remove words with frequency less than 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for word, freq in word_freq.items():\n",
    "    if(freq >= MIN_WORD_FREQ):\n",
    "        vocabulary.append(word)\n",
    "vocabulary.append('<start>')\n",
    "vocabulary.append('<end>')\n",
    "vocabulary.append('<unk>')\n",
    "vocabulary.sort()\n",
    "vocabulary.insert(0, '<pad>')\n",
    "print(len(vocabulary))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving captions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_CAPTIONS_PATH, \"wb\") as file:\n",
    "    pickle.dump(captions, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving vocabulary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCABULARY_PATH, \"wb\") as file:\n",
    "    pickle.dump(vocabulary, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation (on captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the length of the longest caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for _captions in captions.values():\n",
    "    _lengths = list(map(lambda caption: len(caption.split(\" \")), _captions))\n",
    "    max_length = max(max_length, max(_lengths))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word to index mapping and Index to word mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "\n",
    "# word_to_index['<pad>'] = 0\n",
    "# index_to_word[0] = '<pad>'\n",
    "\n",
    "index = 0\n",
    "for word in vocabulary:\n",
    "    word_to_index[word] = index\n",
    "    index_to_word[index] = word\n",
    "    index += 1\n",
    "print(word_to_index)\n",
    "index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving word-to-index and index-to-word mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORD_TO_INDEX_PATH, \"wb\") as file:\n",
    "    pickle.dump(word_to_index, file)\n",
    "with open(INDEX_TO_WORD_PATH, \"wb\") as file:\n",
    "    pickle.dump(index_to_word, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Image Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading training examples from Flickr_8k.trainImages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_IMAGES_PATH, \"r\") as file:\n",
    "    train_images = file.read().strip().split(\"\\n\")\n",
    "print(len(train_images))\n",
    "train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the inception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "backend.clear_session()\n",
    "inceptionv3 = InceptionV3(weights=\"imagenet\")\n",
    "input_layer = inceptionv3.layers[0].input\n",
    "output_layer = inceptionv3.layers[-1].input\n",
    "inception_model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the vgg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "backend.clear_session()\n",
    "vgg16 = VGG16(weights=\"imagenet\")\n",
    "input_layer = vgg16.layers[0].input\n",
    "output_layer = vgg16.layers[-1].input\n",
    "vgg_model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting encodings of the training examples (Inception model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodings(image_path):\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    img = image.img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    img = numpy.expand_dims(img, axis = 0)\n",
    "    print(img.shape)\n",
    "    _encodings = inception_model.predict(img)[0]\n",
    "    return _encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting encodings of the training examples (VGG model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodings(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    img = numpy.expand_dims(img, axis = 0)\n",
    "    print(img.shape)\n",
    "    _encodings = vgg_model.predict(img)[0]\n",
    "    return _encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "progress = tqdm(total=len(train_images), desc=\"Encoded\", position=0)\n",
    "encodings = {}\n",
    "for train_image in train_images:\n",
    "    _encodings = get_encodings(os.path.join(IMAGES_PATH, train_image))\n",
    "    encodings[train_image] = _encodings\n",
    "    progress.update(1)\n",
    "    break\n",
    "# print(len(encodings))\n",
    "print(encodings[train_image].shape)\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving encodings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ENCODINGS_PATH, \"wb\") as file:\n",
    "    pickle.dump(encodings, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "progress = tqdm(desc=\"Extracted\", position=0)\n",
    "_word_embeddings = {}\n",
    "with open(GLOVE_WORD_EMBEDDINGS_PATH, \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        splits = line.split(\" \")\n",
    "        word = splits[0]\n",
    "        embeddings = splits[1:]\n",
    "        _word_embeddings[word] = numpy.array(embeddings)\n",
    "        progress.update(1)\n",
    "print(len(_word_embeddings.keys()))\n",
    "_word_embeddings['the'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = numpy.zeros((len(vocabulary), 100));\n",
    "for word, index in word_to_index.items():\n",
    "    if index == 0:\n",
    "        continue\n",
    "    if word in _word_embeddings:\n",
    "        word_embeddings[index-1] = _word_embeddings[word]\n",
    "#     else:\n",
    "#         word_embeddings[index-1] = numpy.zeros(100)\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving embeddings matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORD_EMBEDDINGS_PATH, \"wb\") as file:\n",
    "    pickle.dump(word_embeddings, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generator function (for Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, captions, image_encodings, word_to_index, vocabulary_length, max_caption_length):\n",
    "    count = 0\n",
    "    _image_encodings = []\n",
    "    input_sequence_vectors = []\n",
    "    output_word_vectors = []\n",
    "    while 1:\n",
    "        for img, _captions in captions.items():\n",
    "            for caption in _captions:\n",
    "                vectorized_caption = []\n",
    "#                 vectorized_caption.append(word_to_index['<start>'])\n",
    "                for word in caption.split(\" \"):\n",
    "                    if word not in word_to_index:\n",
    "                        word = '<unk>'\n",
    "                    vectorized_caption.append(word_to_index[word])\n",
    "#                 vectorized_caption.append(word_to_index['<end>'])\n",
    "                for i in range(len(vectorized_caption) - 1):\n",
    "                    input_sequence_vector = vectorized_caption[:i+1]\n",
    "                    input_sequence_vector = sequence.pad_sequences([input_sequence_vector], maxlen=max_caption_length, dtype='int32', padding='post', value=0)[0]\n",
    "                    \n",
    "                    output_word_index = vectorized_caption[i+1]\n",
    "                    output_word_vector = numpy.zeros(vocabulary_length)\n",
    "                    output_word_vector[output_word_index] = 1\n",
    "            \n",
    "                    _image_encodings.append(image_encodings[img])\n",
    "                    input_sequence_vectors.append(input_sequence_vector) \n",
    "                    output_word_vectors.append(output_word_vector)\n",
    "            count += 1\n",
    "            if count == batch_size:\n",
    "#                 print(numpy.array(_image_encodings).shape)\n",
    "#                 print(numpy.array(input_sequence_vectors).shape)\n",
    "#                 print(numpy.array(output_word_vectors).shape)\n",
    "                yield ((numpy.array(_image_encodings), numpy.array(input_sequence_vectors)), numpy.array(output_word_vectors))\n",
    "                \n",
    "                count = 0\n",
    "                _image_encodings = []\n",
    "                input_sequence_vectors = []\n",
    "                output_word_vectors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "backend.clear_session()\n",
    "input_img = layers.Input(shape=(2048,))\n",
    "dropout = layers.Dropout(0.5)(input_img)\n",
    "dense = layers.Dense(256, activation='relu')(dropout)\n",
    "model_img = dense\n",
    "\n",
    "input_txt = layers.Input(shape=(max_length, ))\n",
    "embedding = layers.Embedding(word_embeddings.shape[0], 100, mask_zero=True)(input_txt)\n",
    "dropout = layers.Dropout(0.5)(embedding)\n",
    "# lstm = layers.VGG(256)(dropout)\n",
    "lstm = layers.LSTM(256)(dropout)\n",
    "model_txt = lstm\n",
    "\n",
    "layer_merge = layers.add([model_img, model_txt])\n",
    "dense = layers.Dense(256, activation='relu')(layer_merge)\n",
    "output = layers.Dense(word_embeddings.shape[0], activation='softmax')(dense)\n",
    "\n",
    "model = Model(inputs=[input_img, input_txt], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the embeddings extracted from a pretrained GloVe model as weights on the embedding layer in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([word_embeddings])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encodings = {}\n",
    "with open(ENCODINGS_PATH, \"rb\") as file:\n",
    "    image_encodings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = {}\n",
    "for train_image in train_images:\n",
    "    if train_image in captions:\n",
    "        train_captions[train_image] = captions[train_image]\n",
    "print(len(train_captions))\n",
    "train_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 10\n",
    "\n",
    "epochs = 200\n",
    "history = model.fit_generator(data_generator(batch_size=batch_size, captions=train_captions, image_encodings=image_encodings, word_to_index=word_to_index, vocabulary_length=len(vocabulary), max_caption_length=max_length), \n",
    "                        steps_per_epoch=int(len(image_encodings)/batch_size), \n",
    "                        epochs=epochs, \n",
    "                        verbose=1)\n",
    "model.save(MODEL_PATH)\n",
    "with open(MODEL_HISTORY_PATH, \"wb\") as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ENCODINGS_PATH, \"rb\") as file:\n",
    "    encodings = pickle.load(file)\n",
    "with open(PROCESSED_CAPTIONS_PATH, \"rb\") as file:\n",
    "    captions = pickle.load(file)\n",
    "with open(WORD_EMBEDDINGS_PATH, \"rb\") as file:\n",
    "    word_embeddings = pickle.load(file)\n",
    "model = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beamSearch(test_image_path, beam_index, model):\n",
    "    test_image_encodings = get_encodings(test_image_path)\n",
    "    input_sequence = [[[word_to_index['<start>']], 0]]\n",
    "    while 1:\n",
    "        top_words = []\n",
    "        for _seq in input_sequence:\n",
    "            input_sequence_vector = sequence.pad_sequences([_seq[0]], maxlen=max_length, padding=\"post\")\n",
    "            prediction = model.predict([numpy.array([test_image_encodings]), numpy.array(input_sequence_vector)])[0]\n",
    "            top_predictions = numpy.argsort(prediction)[-beam_index: ]\n",
    "            \n",
    "            for word in top_predictions:\n",
    "                next_captions = _seq[0][:]\n",
    "                probability = _seq[1]\n",
    "                next_captions.append(word)\n",
    "                probability += prediction[word]\n",
    "                top_words.append([next_captions, probability])\n",
    "        input_sequence = top_words\n",
    "        input_sequence = sorted(input_sequence, reverse=True, key=lambda x: x[1])\n",
    "        input_sequence = input_sequence[:beam_index]\n",
    "        if len(input_sequence[0][0]) >= max_length:\n",
    "            break\n",
    "    input_sequence = input_sequence[0][0]\n",
    "    predicted_caption = []\n",
    "    for i in range(len(input_sequence)):\n",
    "        if input_sequence[i] == word_to_index['<end>']:\n",
    "            predicted_caption.append('<end>')\n",
    "            break\n",
    "        predicted_caption.append(index_to_word[input_sequence[i]])\n",
    "  \n",
    "    return (\" \".join(predicted_caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_IMAGES_PATH, \"r\") as file:\n",
    "    test_images = file.read().strip().split(\"\\n\")\n",
    "print(len(test_images))\n",
    "# test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor(s):\n",
    "    s = s.replace(\"<start>\", \"\").replace(\"<end>\", \"\").replace(\"<unk>\", \"\").strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "progress = tqdm(total=len(test_images[:25]), desc=\"Tested\", position=0, leave=True)\n",
    "\n",
    "res = numpy.zeros((len(test_images[:25]), 3, 9), dtype='float32')\n",
    "for i, test_image in  enumerate(test_images[:25]):\n",
    "    test_image_path = os.path.join(IMAGES_PATH, test_image)\n",
    "\n",
    "    # plot.imshow(plot.imread(test_image_path))\n",
    "    # plot.axis('off')\n",
    "    # plot.show()\n",
    "  \n",
    "    for j, model in enumerate([model_100d, model_200d, model_300d]): \n",
    "        for k, beam_index in enumerate([1, 2, 3, 4, 5, 6, 7]):\n",
    "            cap1 = beamSearch(test_image_path, beam_index, model)\n",
    "  \n",
    "            cap1 = refactor(cap1)\n",
    "  \n",
    "            _captions = []\n",
    "            for caption in captions[test_image]:\n",
    "              _captions.append(refactor(caption))\n",
    "  \n",
    "            score1 = sentence_bleu(_captions, cap1)\n",
    "  \n",
    "            res[i][j][k] = score1\n",
    "  \n",
    "        progress.update(1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_RESULTS_PATH, \"wb\") as file:\n",
    "    pickle.dump(res, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
